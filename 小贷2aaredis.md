持久化 原子操作 异步复制

**介绍一下redis数据库？**（1.单线程，内存型，读写快2.数据结构3.用途：缓存消息队列分布式锁4.支持持久化，集群）

Redis 是一种基于内存的数据库，对数据的读写操作都是在内存中完成，因此**读写速度非常快**，常用于**缓存，消息队列、分布式锁等场景**。

Redis 提供了多种数据类型来支持不同的业务场景，比如 String(字符串)、Hash(哈希)、 List (列表)、Set(集合)、Zset(有序集合)、Bitmaps（位图）、HyperLogLog（基数统计）、GEO（地理信息）、Stream（流），并且对数据类型的操作都是**原子性**的，因为执行命令由单线程负责的，不存在并发竞争的问题。

除此之外，Redis 还支持**事务 、持久化、Lua 脚本、多种集群方案（主从复制模式、哨兵模式、切片机群模式）、发布/订阅模式，内存淘汰机制、过期删除机制**等等。

Redis常见的**应用场合**有：

- 缓存，消息队列，排行榜或计数，消息发布和订阅，商品列表

### **数据结构及其底层实现**

string：sds/int

list：双向链表/压缩链表 -> 已改为quickList

hash：哈希表/listPack

set：哈希表/整数集合

zset：哈希表 + 压缩列表/跳表 -> 已改为listPack/跳表



**string（字符串,整数,浮点数），List（列表），Set（集合），Hash（哈希），Zset（有序集合）**

String 类型的应用场景：二进制安全，能储存任何数据，缓存对象、常规计数、分布式锁、共享 session 信息等。
List 应用场景：消息队列(如何保证消息有序，不重复，可靠性)，生产者需要自行实现全局唯一 ID，不能以消费组形式消费数据
Hash 类型：缓存对象、购物车等。
Set 类型：聚合计算（并集、交集、差集）场景，比如点赞、共同关注、抽奖活动等。
Zset 类型：排序场景，比如排行榜、电话和姓名排序等。底层：跳表（改进的多层有序链表）



#### String

**SDS（简单动态字符串simple dynamic string）**

因为传统的c语言字符串是以'/0'结尾，这样导致很多问题，求长度需要遍历，拼接字符串是遍历到尾端然后继续写新的，这样可能导致缓冲区溢出，读到'/0'就认为是结尾了，长度可能计算错误，无法储存二进制数据。

sds结构设计：一个struct(len, alloc, flags, buf[])（当前长度，总空间，sds类型，实际数据）



因为有len记录长度，所以不需要通过'/0'判断结尾，因此是二进制安全的，可以储存图片，视频，音频等数据，同时求长度快
有缓冲区大小，防止 strcat等字符串追加函数导致缓冲区溢出，所需内存小于1MB，翻倍扩容，之后一次加1MB，同时原来的strcat函数需要遍历找到才能找到字符串结尾，然后进行追加，处理效率不高

sds类型的区别在于struct中的len和alloc结构类型不同，标识最大长度和分配空间，同时使用了专门的编译优化，**取消结构体在编译过程中的优化对齐，按照实际占用字节数进行对齐**。从而节省内存

#### List

可以储存重复元素，按先后顺序储存元素

压缩列表或双向链表（redis的链表是双向的，有一个节点数量字段）。后改为只使用**quicklist** (双向链表，节点是一个压缩列表，如果插入空间不够只需要新建一个节点)

**压缩列表**

区分链表，不是通过指针实现，而是通过字节数

用一块连续的内存空间，提高内存效率。但又不能像数组一样直接定位，每个操作都需要重新分配 ziplist 使用的内存，因此实际复杂性与 ziplist 使用的内存量有关。

O(1)找头尾，可以头尾双向遍历，根据表头字段实现（储存了尾距离头多少字节），每个节点存放当前节点的长度和前一个节点的长度（用于后续遍历）和实际数据，存在连锁更新问题

优点是节省内存，缺点是只会用于保存节点数量不多的场景

![image-20250224115430661](C:\Users\16776\AppData\Roaming\Typora\typora-user-images\image-20250224115430661.png)

quickList：

![image-20250224114632055](C:\Users\16776\AppData\Roaming\Typora\typora-user-images\image-20250224114632055.png)

listPack：哈希表和set，zset会用

仍然是连续的内存空间，每个节点包含编码类型+实际数据+当前节点长度（取消了记录前一个节点的长度字段，也不需要记录尾）

![image-20250224114959835](C:\Users\16776\AppData\Roaming\Typora\typora-user-images\image-20250224114959835.png)

#### set

无序且唯一，可以交并差；常用于数据去重或保证数据唯一性

交并差复杂度较高，大数据量为防止主库阻塞，应交由从库统计或客户端统计

时间复杂度：增删查都是o(1)，交并差是o(n)

#### zset

+ 用途

和set一样不能重复，但多了score可以按分数排序(以插入顺序的角度，仍是无序的)

分数相同，字典序排序；因此可以给电话号码排序，给姓名排序

+ 底层：哈希表+跳表：

有序集元素个数小于128且每个元素的值小于64字节时用压缩列表（后改为listPack）否则使用跳表

哈希表用于常数时间获取score（key是member，val是score）

跳表用于score排序（基于分数进行跳表的插入删除操作）

+ 时间复杂度：查o(1)，哈希表直接判断是否存在，也能返回分数；

  增删要同时操作哈希表和跳表，所以是o(logN)

+ **跳表**

再结合哈希表为什么能排序？

跳表是多层有序链表，底层包含所有元素，是双向链表(为了范围查找)

某节点层高为n，level数组大小就为n

头节点层高为k，先从最高层尝试向后，如果发现大了，转为下一层尝试向后（设计灵感来源平衡树）

跳表时间复杂度，最好相邻两层2：1；总共n个节点，最后一层节点数≈1，那么层高k = log2(n)，每层最多查两个节点（如果查比两个多应该是上层向后移动一次），所以时间复杂度2log2(n)

为什么用跳表：实现简单，查找快(相比于链表)，方便实现zrange(相比于树)

```
typedef struct zskiplistNode {
    //Zset 对象的元素值
    sds ele;
    //元素权重值
    double score;
    //后向指针
    struct zskiplistNode *backward;
  
    //节点的level数组，保存每层上的前向指针和跨度
    struct zskiplistLevel {
        struct zskiplistNode *forward;
        unsigned long span;
    } level[];
} zskiplistNode;
```

每个Node包括自己这个节点和一个level数组，数组中的每个节点有两个属性(指向下一个节点的指针和跨度)

当前在第k层，根据当前位置i，根据跨度，确定下一个点的score，如果大了，就去level[k-1]看跨度





**跳表和链表**

链表查询复杂度on，查询效率低，所以产生了跳表

跳表就是有序表加了个多级索引，所以查找比原来更快，同时因为双向链表，增删也很快，也能范围查

**跳表和二叉平衡树**

查的时间复杂度一样，但：1.跳表实现更简单，不需要复杂的平衡调整操作，2.维护更简单，通过控制层级，更容易控制内存消耗，3.**范围查找效率更高**

空间复杂度：如果2：1，有1/2的概率只有一层，有1/2*1/2的概率只有两层，有1/8的概率只有三层..总指针数大概一点几个，不到2

**跳表和红黑树**

实现简单、好插入不用翻转，好实现range

#### map

底层实现：压缩列表（个数小于512，已改为listpack，先来一个节点储存key在紧跟一个节点储存val）或哈希表

哈希实现：数组做哈希桶，链式哈希解决哈希冲突，不转红黑树，但链太长触发rehash

结构设计：一个struct，包括哈希表数组指针，哈希表大小，已存放的大小

**rehash**

定义两个哈希表，交替使用，表一储存数据，表二没有分配内存，随着表一空间增大，触发rehash，给表二分配空间，大小为表一的二倍，把表一的数据迁移到表二，释放一的内存，最后把表二置为表一，1置为2。

为了防止hash中数据过大导致迁移时服务阻塞的问题，又提出**渐进式rehash**

**渐进式rehash**

为了防止阻塞，提出渐进式rehash扩容，迁移工作分多次完成，每次增删改查的时候除了执行操作，还会移动到表2上，保证1的数据只会减少，随着请求操作，最终把所有数据迁移到表二，增删改查会同时在两个表执行，如查的时候先去表1查，表1查不到去表2

补：是否触发通过计算负载因子：保存的节点数除以哈希表大小

补：为什么是扩容二倍：减少扩容时的迁移，（16扩容32）将原来的哈希值与新增加的容量（16）进行&运算，如果为0就不用扩容，否则原来的位置加16



**String和hash存对象：**

hash便于修改单个字段，能节省网络流量，如果字段经常变动或者经常查单个字段信息，用hash

string消耗的内存更少，缓存相同数量的对象数据，String 消耗的内存约是 Hash 的一半，储存**嵌套对象**也很方便，多数情况下用string

**set的底层实现**：整数集合（元素个数小于512）或 哈希表

### 单线程相关

**Redis里面的操作是原子操作吗，如何实现的**？

 单线程，支持事务，数据结构操作也是原子的（如：set key  value，给list push 元素）

**说说你对Redis操作原子性的理解**

redis是单线程模型的，命令的执行过程不会涉及线程切换，整个命令的执行不会被中断，要么全部执行成功要么全部失败

**redis事务**

redis的事务是将所有命令序列化然后按顺序执行，期间不会被其他命令打断，但是如果有一条命令执行失败不会回滚，而是会执行后面的命令。（redis没有回滚的概念），因此不保证原子性

本质上是最后执行一个批处理

#### K) **Redis 是单线程吗？**

（多核机器里使用会不会浪费机器资源？）

主要工作单线程（解析请求，进行读写），6.0之后，io处理多线程，**持久化，异步删除，集群数据同步**使用后台线程（直接使用del是同步删除，但也有异步删除指令）

Redis 单线程指的是主要工作是单线程，即「接收客户端请求->解析请求 ->进行数据读写等操作->发送数据给客户端」这个过程是由一个线程（主线程）来完成的。支持高并发是因为结合io多路复用，一个线程监听多个socket（监听soc和已连接soc），通过事件循环函数
6.0之后采用多个io处理线程处理网络请求建立socket请求）

Redis 程序并不是单线程的，Redis 在启动的时候，是会启动后台线程的（持久化，异步删除，集群数据同步）

**Redis为啥单线程还这么快**（内存，单线程，多路复用）（处理io请求是多线程，命令执行是单线程）

内存存储、单线程模型、高效的数据结构、io多路复用，事件驱动的非阻塞

内存型数据库，读写快（性能瓶颈在内存和网络带宽，不在cpu，也就不需要多线程）。

单线程模型，简化并发控制，避免多线程竞争与同步开销（加锁解锁，上下文切换），没有死锁问题

非阻塞io结合io多路复用（epoll）处理大量客户端socket请求（一个线程处理多个io流）。

**优化：批量操作，管道技术**，减少了网络通信开销

补： Redis 6.0 版本之后，因为网络硬件能力提升，有时候redis的性能瓶颈为网络io处理，所以也采用了多个 I/O 线程来处理io请求，对命令的执行仍然是单线程，多路复用在调用epoll的过程是阻塞的，高并发会成为性能瓶颈

**那如何利用多核心？**docker部署多个redis



### 过期删除和内存淘汰

普通key空间，dict，储存了数据库中所有的k-v；

过期字典额外存了设置过期时间的(相当于存了第二份)，里面有过期时间信息，是绝对时间(时间戳)，和当前时间比较判断是否过期以及还剩多长时间

如何判定已过期，有个过期字典(k指向key，val是过期时间)，查key先去过期字典查有没有，没有直接返回；有判断过期时间；

#### **Redis过期删除策略**

（有什么？redis用的是什么？）

**定时删除：**设置过期时间的同时创建定时事件

​	优点：尽快删除，内存友好

​	缺点：占用cpu，可能导致将 CPU 时间用于删除和当前任务无关的过期键上，导致吞吐量低

**惰性删除：**不主动删除过期key，每次访问时检查是否过期，若过期则删除，返回null

​	优点：系统资源占用少 ，cpu友好

​	缺点：主线程执行；如果过期的key一直没被访问，就会一直留在库中，对内存不友好

**定期删除：**每隔一段时间取出一定数量的key检查，删除过期的。

​	优点：后台线程执行；中庸，通过控制时长和频率，减少对cpu的影响，也一定程度减少了对内存的影响

​	缺点：难以确定执行频率，太短对cpu不好，变成定时删除；太长变成定期删除

**redis的选择：**惰性+定期

​	惰性：访问的时候看是否过期，过期就删了

​	定期：定期抽查，取若干key，删除过期的，并记录过期个数，如果发现过期比例过高会再次执行。

​	为了确保不会循环过度，导致线程卡死，增加了一个执行时间上限

#### **Redis内存淘汰策略**

（如何保证redis中都是热点数据）（内存不够了咋办）

del是主线程执行，但是想删大key可以用unlink命令，给个逻辑过期，后台线程去删

后台线程执行

**内存淘汰：**允许在资源紧张时，根据一些策略，主动删除一些键值对，以释放空间保证系统稳定性

不淘汰策略，空间不足禁止写入(可以读)，返回错误，写操作执行失败

在设置过期时间中的数据淘汰：random，ttl，lru，lfu

在所有数据范围淘汰：random，lru，lfu

**redis的lru是如何是实现的？**

为什么没用链表，因为会消耗额外空间，加移动耗时

近似lru：给对象加个字段，记录最后一次访问时间，随机取几个值，最久没用的淘汰

缺点：缓存污染，一次读了大量数据，但只用这一次

lfu：对象加个字段，但不是单纯记录访问次数，而是记录频率，一个随时间衰减的值

lru和lfu用的是一个字段，24位，lru中全用来表示时间；lfu中高16位记录时间，第八位储存逻辑计数

### **日志**

#### 持久化

**Redis里的数据落地(持久化)机制**

（AOF存命令，文件大，恢复慢，丢失少）（rbd存数据，文件小，恢复快，丢失多）

默认开启rbd，满足条件就执行。如900秒内至少修改一次，300秒修改10次，60秒修改10000次

默认不开启AOF

**AOF日志**：

如何实现持久化：每执行完一条写命令，把命令以追加的形式写到一个文件

​		无需额外检查，不会阻塞当前命令执行

具体流程：命令追加到server.AOF_buf(用户区) -> write() -> pageCache(内核区) -> fsync -> 磁盘

所以产生了三种写回策略(可配置参数)，对应fsync的执行时机

Always：write后，立刻执行fsync，执行完成再返回（主线程执行）

Everysec：创建一个异步任务，每秒执行（创建一个线程，每秒执行一次fsync）

No：交由操作系统执行fsync，时机随机（操作系统调度的线程执行）

**AOF重写**：

避免AOF文件越写越大，在重写时，获取所有的k-v，用一条新命令记录到新的AOF文件。

具体过程：创建**子进程**（不是线程）避免阻塞，父子进程**共享内存**但只读，主进程修改时发生**写时复制**，创建独立的数据副本进行修改，

重写期间，redis执行一个写命令，**同时**将命令写到AOF缓冲区和AOF重写缓冲区，完成后像主进程发信号，将重写缓冲区的内容**追加**到新AOF，最后改名覆盖现有的AOF

补：fork创建子进程会复制页表（虚拟内存到物理内存的映射）

补：写时复制，多任务访问相同的数据用同一个数据而非创建多个数据副本，一旦有一个任务决定修改数据，系统才会为该任务创建独立的数据副本

**RBD快照**：把某个时间点的数据以二进制的方式保存到硬盘的一个文件中，而非操作命令。

恢复效率更高，但操作频率太高会对redis性能产生影响，频率太低会丢失数据。

执行快照的时候可以修改数据，通过写时复制实现主进程复制一份，用新数据；子进程用原来的旧数据，如果恰好

在生成完rbd后崩溃了，会丢失这期间修改的所有数据。

可以定时，可以手动，可以设置触发条件

**混合持久化**：AOF丢失数据少，恢复慢，rbd丢失数据多，恢复快

混合持久化工作在AOF日志重写这一过程，重写子进程时先将数据以Rbd的形式写到AOF文件，这个过程中的写操作记录到AOF重写缓冲区，再将这部分增量命令以命令格式写到AOF

**大key对持久化影响**

AOF持久化，使用fsync时，如果用了Always，需要等待很长时间的大key拷到磁盘的过程，如果是Everysec，因为是异步执行，所以不影响主线程工作，None也不影响主线程工作

aof和rbd，在父进程修改共享内存大key时，都有写时复制，耗时，会阻塞父进程

除影响持久化外，还会导致网络阻塞(io)，以及工作线程阻塞(del大key，或其他修改)

#### 主从复制

**Redis主从复制**

（为什么要用？如何实现？缺点是什么）（redis新节点加入集群会发生什么）

防止单点故障，一个机器坏了，其他服务器仍然可以提供服务

redis用了读写分离：主服务器可以读写，以及进行写传播；从服务器只读

**具体流程：**

建立长连接 -> 全量同步(rdb+新写传播) -> 增量同步 -》增量主要靠环形缓冲区，从发送offset

全量复制： bgsave 命令来生成 RDB 文件（生成子进程，不会阻塞主线程工作），生成rb，发送，从服务器接收并加载；(虽然是子进程，拷贝的时候不会阻塞；但fork的时候会阻塞)

写传播：这个过程中的新的写命令，会存到 **replication buffer 缓冲区**里，从服务器加载完rbd会回复一个确认，然后主服务器把缓冲区里的命令发送给从服务器，此时第一次同步完成

增量同步：双方维护一个tcp长连接，主服务器有一个环形缓冲区（repl_backlog_buffer），主从各自维护一个

offset，主记录自己写到的位置，从节点记录自己读到的位置，然后从把自己的offset回复过去，插值就是需要发

送的，如果环形有的区域被覆盖，该从节点就要重新全量复制。（replication buffer 会根据offset清除没用的）

写命令会同时写到 replication buffer 和 repl_backlog_buffer， 

主服务器给每个从节点都分一块replication buffer（先写到内部缓存），然后分别把内容拷到对应的内核socket发送缓冲区。所以是异步发送

但主服务器只有一个repl_backlog_buffer，多从共用。

为了避免网络恢复时，主服务器频繁的进行全量同步，可以调大 repl_backlog_buffer，大小为平均断联时间*每秒发送的消息大小

补：一个叫复制缓冲区（实现隔离？），一个叫复制积压缓冲区（环形），

 **主从复制中两个 Buffer(replication buffer 、repl backlog buffer)有什么区别？**

replication buffer 、repl backlog buffer 区别如下：

- 出现的阶段不一样：
  - repl backlog buffer 是在增量复制阶段出现，**一个主节点只分配一个 repl backlog buffer**；
  - replication buffer 是在全量复制阶段和增量复制阶段都会出现，**主节点会给每个新连接的从节点，分配一个 replication buffer**；
- 这两个 Buffer 都有大小限制的，当缓冲区满了之后，发生的事情不一样：
  - 当 repl backlog buffer 满了，因为是环形结构，会直接**覆盖起始位置数据**;
  - 当 replication buffer 满了，会导致连接断开，删除缓存，从节点重新连接，**重新开始全量复制**



**如何解决主从不一致**

为什么会不一致？因为写传播是异步的

保证网络良好，主从放同一个机房

计算每个从节点与主之间的offset，大于阈值，不让从这个从节点读取数据。

**主从会丢失数据吗**

主写完，返回成功，命令没来得及发送，从还没来的及同步，主宕机

**如何减少主从切换数据丢失**

异步复制导致丢失

同步所需时间超过阈值，认为故障丢失数据会很多，禁止向主节点写数据

对于客户端，当客户端发现 master 不可写后，我们可以采取降级措施，将数据暂时写入本地缓存和磁盘中。也可以写到kafka，等master恢复正常消费

集群脑裂导致丢失



**其他：**

redis有四个缓冲区：

aof缓冲区（用于持久化），aof重写缓冲区（用于aof重写），replication buffer（用于全量同步中的写传播和增量同步），repl_backlog_buffer（用于增量同步）

**如何判断节点是否正常工作**

主给从每10s发ping，确定从在线

从给主每1s发offset，确定主在线，同时用于增量同步





**Redis 如何实现服务高可用（redis宕机怎么办）**

**1.主从复制，读写分离**，数据修改只在主服务器进行，收到写命令，发给从服务器。但是，主服务器并不会等到从服务器实际执行完命令后，再把结果返回给客户端，而是主服务器自己在本地执行完命令后，就会向客户端返回结果了。如果从服务器还没有执行主服务器同步过来的命令，主从服务器间的数据就不一致了（异步的）（不是强一致性）

2.**哨兵模式**：监控主从服务器，提供主从节点故障迁移的功能（ping），哨兵节点主要负责三件事情：**监控（ping）、选主、通知**。

3.**切片集群**：数据量大到一台服务器无法缓存，将数据分不到不同服务器上，形成集群，一个集群16384个哈希槽，通过哈希槽处理数据和节点之间的映射关系

**脑裂问题**

主从复制存在集群脑裂问题，可能导致数据丢失

由于网络问题，集群节点之间失去联系。主节点仍能接收请求，但无法像从节点复制，主从数据不同步；哨兵重新组织选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，会清空自己的缓冲区，导致之前客户端写入的数据丢失了。

解决方法：主库连接的从库中至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则，主库就不会再接收客户端的写请求了。

#### 哨兵

哨兵和节点间：哨兵主动ping，节点不回，认为主观下线。哨兵自己成leader，找其他哨兵投票，判断是否客观下线。leader哨兵选集群新主，通知客户端主更换了

哨兵间通过redis发布订阅互相发现，都订阅同一个频道(频道在redis集群主节点)，把自己的ip端口发布。哨兵和其他redis间的连接建立是靠从主节点获取相关信息

#### 水平扩展

原因：单机容量有限

客户端分片：分片逻辑放在客户端实现，一般根据key算hash，根据计算结果找对应的节点

优点：实现简单，数据分布和访问完全由客户端控制，灵活性高

缺点：客户端要维护节点信息，节点数目发生变化，要重新计算分片，涉及大量数据迁移，无法实现自动的负载均衡和故障转移

redis cluster分片：redis官方提供的，就是一致性哈希。使用 `redis-cli --cluster create` 命令将 16384 个哈希槽分配到各个节点上。客户端可以连接到集群中的任意一个节点，通过该节点进行数据的读写操作。

有个集群管理器，不是redis节点，不存数据，帮助用户执行诸如创建集群、添加节点、删除节点、重新平衡哈希槽等操作。集群管理器命令源节点把要迁移的数据迁移到目标节点

优点：自动的分片和负载均衡；故障转移交给集群来进行

缺点：受网络延迟和集群节点间通信影响大

代理分片：客户端和redis间加个代理层

#### 一致性哈希

正常哈希如果扩容或者某个节点挂了，就会出现key找不到需要的数据，就需要大量数据迁移

一致性哈希，通过哈希槽(2^14)，一个节点挂了或增加，被影响的数据少了

再配合虚拟节点提高均衡度，哈希后可能落到虚拟节点，虚拟去找真实

#### redis集群有哪几种

哨兵+主从，哨兵选主

分布式+一致性哈希(cluster)，分布式主下面也可以有从，从请求其他几个主让自己成为主



### 其他

#### **缓存雪崩、缓存击穿、缓存穿透？**

**缓存雪崩**：**大量缓存数据在同一时间过期或者 Redis 故障宕机**，若此时有大量用户请求，都无法在redis处理，全部冲向数据库，数据库压力骤增，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃。
解决方案：**1.将缓存失效时间随机打散**（在原有时间基础上加一个随机值）
**宕机：2.**服务熔断（暂停访问，直接返回错误）请求限流（只接收少量请求发送到数据库）**3.**构建集群，主节点故障切换到从节点

**缓存击穿**：**热点数据**过期，并且重建比较复杂（缓存可能涉及多张表及运算最后缓存一个结果）

解决方案：**1.互斥锁（分布式锁）**，线程1查缓存未命中，查数据库，构建缓存，构建过程中，线程2，3，4也未命中，都去数据库并构建缓存，导致数据库访问压力大，所以用互斥锁和双重检测给构建缓存这个过程加锁，线程1加锁构建缓存，线程234拿不到锁，休眠一会重试，等拿到锁进行**二次检测**，检测当前缓存是否存在，若存在直接返回（锁也要加过期时间，防止发生意外一直不释放）
 **2.设置逻辑过期时间，**过期时间不作用于redis，而是从value中判断当前数据过期了（把原来打算存的类和一个过期时间封装成新的类，当作新的val），线程1发现过期，加锁，创建一个新线程**异步更新**数据，更新完了再返回，线程234也发现过期了，但获取不到锁，直接返回旧数据  --------- 黑马点评中是先加分布式锁，然后给线程池提交任务，得不到锁返回null

**缓存穿透**：要访问的数据**不在缓存也不在数据库**，没法构建缓存来服务后续请求（可能是误删除或黑客恶意请求不存在的信息）

解决方案：**1.**限制非法请求（api入口处判断请求参数是否合理，是否有非法值，请求字段是否存在）
**2.**缓存空值，这样就不会继续查数据库（问题：占用空间，缓存大量非法数据，可能造成数据不一致（因为后续可能插入了这个数据），所以要设置ttl）
**3.**使用布隆过滤器快速判读数据是否存在，判断不存在就不用访问数据库（redis本身支持布隆过滤器，存在误判）需要将所有可能存在的key预先加载到过滤器中

其他：多级缓存（本地缓存+分布式缓存）高可用架构（主从复制加集群，避免单点故障）

补：双key策略，主key设置过期时间，备key永久，主过期，返回备key得内容

**缓存预热：**

业务刚上线或redis宕机恢复的时候，我们最好提前把数据缓起来，而不是等待用户访问才来触发缓存构建，这就是所谓的缓存预热

**Redis如何实现分布式锁**

分布式锁：**分布式系统或集群模式下**，多线程可见并互斥的锁，实现的核心思想是大家都使用一把锁

举例：一人一单问题，需要先根据用户id加锁，拿到锁对象，去数据库找有没有用户id相同且订单id相同，下单，释放锁

问题：分布式环境下，用户多次点击，请求到不同的服务器，每个服务器都能加锁（锁对象不同），就会导致多次下单

为什么用redis：redis本身**被多个客户端共享**，多进程可见，可以用来保存分布式锁，读写性能高，可以应对高并发

原理：SET lock_key unique_value NX PX 10000 

例如:  key是lock:用户id（一人一单），val是uuid+线程id（不同服务器线程id可能相同）

三个要求1.读取锁变量、检查锁变量，设置变量值要以原子操作完成

2.设置过期时间，防止客户端发生异常，无法释放锁

3.解锁就是把锁删除，要保证删锁的客户端就是加锁的客户端，防止误释放，包括两个操作（1.检验，2.删除）需要lua脚本保证原子性

误删：持有锁的线程1阻塞，没有完成任务，没有触发解锁，但到了时间自动释放锁，线程2得到锁；线程1恢复，继续执行，最终会把线程2的锁释放

**分布式锁怎么解决缓存击穿的**

击穿：热数据过期，多个分布式服务去访问数据库想要重新构建缓存

一个请求发现缓存不存在，首先尝试获得分布式锁，如果拿到锁，进行二次检查，缓存仍不存在则访问数据库构建缓存

**分布式id或者说全局唯一id**

需要有一个分布式环境中的唯一标识符，标识各种实体。

如果使用主键自增会有一些问题1.规律性太明显，可能被猜到信息（一天卖了多少单）2.数据量大需要分表，分表后逻辑上仍然是一张表，需要保证id唯一性和递增性。

**UUID**：值是随机的，导致随机插入，页分裂，影响查询性能

**Snowflake算法**（雪花）：时间戳、数据中心ID、机器ID和序列号

**redis/数据库自增ID:**

**数据库与缓存双写一致性**

改缓存和改数据库一定要同时成功或失败，所以必须**事务操作**

为什么是删除缓存而非更新缓存（1.防止一直没有使用，但不断发生更新数据库，从而导致不断更新缓存2.可能有并发问题，两个线程同时更新数据库，更新缓存，执行顺序不同，结果不同3.缓存可能通过复杂计算得到，如果更新频率高会浪费性能）

一定是先更新数据库再删缓存（删缓存的动作很快，被别的线程打断这一动作的概率小，就不会从缓存读到与数据库不一致的数据）（缓存恰好过期（或缓存不存在），读旧数据但没来的及写缓存，新线程改了数据库，改缓存，旧线程又切换回来，把旧数据写入缓存）

先删缓存，再更新数据库（可能刚删完缓存还没有更新数据库，此时发生线程切换，新线程访问发现没有数据，去数据库中找，然后用旧数据构建了缓存，导致不一致，发生的概率较大，因为删缓存是一个较快的动作，改数据库是一个较慢的动作）（删缓存，写数据库），（查缓存，读数据库）后者执行快，前者慢

**补：延迟双删**：先删缓存再删数据库，延迟N秒再删一次缓存

**如何实现按照积分降序排序，按照时间升序排序**

把时间信息和积分想办法编码到一起，用于分数，按socre降序，所以要想办法让时间小的反而分数高，可以指定一个时间，可以让时间变成负数

**布隆过滤器**

一种数据结构，解决海量数据的存在性问题且容忍轻微误差这一场景，对缓存穿透、海量数据去重这一场景很适合

比list，map，set占用空间更少，效率更高

具体实现：用一个较大的bit数组（位图）保存所有数据，每个位的值只能是1，**用多个哈希函数**，对指定元素做哈希计算，得到哈希值对位图数组长度取模，得到位置，将对应位数组置为1，判断是否存在时，就重新进行相同的哈希计算，看是否所有位都为1（说存在可能为误判，说不在则一定不在）

补：哈希函数：随机映射函数，常用取余的方式

**如何解决热key问题**

热 Key 问题是指在缓存系统中，某些特定的缓存key受到高频访问，导致对这些热门数据的读取/写入操作集中在少数几个缓存节点上，使得这些节点的负载过高

解决方法：1.缓存预热，业务低峰期提前加载热门数据 2.一致性哈希，尽可能把热数据分到不同接待你 3.数据分片，让热key尽可能均分

**说说你对PIPELINE的理解**

用于批处理传送命令，减少io次数，适合大量请求，对延迟不敏感的数据

**哨兵机制**：自动发现故障并完成主节点的切换并通知应用方

作用：监控（ping）、故障转移、通知

1.leader选举 2.主观下线--客观下线 3.ping过滤调所有的下线节点，从剩下的节点选优先级最高，复制偏移量最大的

4.leader完成新主接待你的切换 5.通知客户端重定向

**高并发场景下我们如何保证幂等性**？

分布式id当主键而非自增id 2.乐观锁实现幂等性 3.分布式锁 4.token当key

补:**Redis健壮性**,健壮性指处理异常输入或发生故障情况的能力

从可持久化，主从复制，哨兵，内存管理，原子操作来讲

**geo怎么存地理位置的**

底层数据是一个zset，完成了从**经纬度到score**的映射，储存是key 经纬度 member（店铺，也是val）这样的结构，可以计算两点间距离，可以根据member查坐标，可以给定中心，按指定范围（方形或圆形）搜索member，并按距离排序返回

**HyperLogLog**

提供不精确的去重计数，并且不需要储存所有值，多用于UV统计

**redis写操作日志记录什么**

记录操作类型，例如 SET、DEL、HSET 等，指示执行的具体操作是写操作

记录被操作的键（Key）和相应的值（Value），包括写入操作的目标键和相应的值内容。

「`*3`」表示当前命令有三个部分，每部分都是以「`$+数字`」开头，后面紧跟着具体的命令、键或值。然后，这里的「`数字`」表示这部分中的命令、键或值一共有多少字节。例如，「`$3 set`」表示这部分有 3 个字节，也就是「`set`」命令这个字符串的长度。

记录时间戳，写操作发生的时间，以便进行操作追溯和日志分析。？

记录执行写操作的客户端信息，包括客户端IP地址、连接信息等，以便进行操作追踪和监控？

**和关系型数据库的区别**

**CAP原理**（consistent,available,partation tolerance）

分区发生时，一致性和可用性难以两全

#### redission/分布式锁

分布式锁，可重入锁，可续期

利用set nx ex，设置过期时间，val设置为线程id

可重入通过val设为线程id+重入次数实现

续期通过看门狗实现，起一个后门线程，每隔有效期的1/3检查锁是否仍被持有，是的化延长有效期

删除锁需要先get判断是不是当前线程，是的话删除，要保证原子操作，所以还需要lua

**加完锁服务器挂了咋整？**其他线程加不了锁，设个过期时间，配合看门狗续期；防止锁误释放，val放线程id；

**加完锁redis挂了咋整？**redis集群+红锁，需要在超过一半的节点上加锁成功才加锁

**补：**zk怎么实现分布式锁？锁节点下创建临时节点，判断自己创建的节点是否是需要最小的，是的化解锁成功，不是的话监听比自己小的前一个节点的删除事件，该节点删除后，再次判断自己是否是最小的

#### 数据库缓存一致性

**Cache-Aside**

旁路缓存：写：先更新数据库再删缓存（旁路：绕过缓存，直接到数据库）

优点：实现简单，因为本来也要更数据库，就多个删除就行。

缺点：写操作后，缓存失效前（写操作后缓存失效前，可能读到旧数据），会存在短暂不一致，会有**缓存击穿**

适合**读多写中**，如商品详情页

为什么先数据库后缓存，防止：删完缓存 数据库没来的及更新，旧的又被写进去了，数据库更完认为缓存删过了（先删缓存），之后到下次更新都是不一致

补：延迟双删，先删缓存，再更数据库，等一会再删一次缓存

**read/Write-Through**：

读写穿透，应用只和缓存交互

读：有缓存直接读；没缓存，去数据库读，再加载到缓存（所以叫读直达）；

写：先改缓存，再改数据库，如果没缓存，直接写到数据库（无论有无缓存，都更数据库，所以叫直达）

同步更新强一致性，适合余额等场景；每次都要同时写缓存和数据库，开销大（写延迟高），适合**读多写少**，没有删除缓存的措施，冷数据可能长期占用缓存

**write-Back：**

读：始终读缓存

写：先写入缓存，缓存将数据标记为脏数据，在合适的时机（如缓存空间不足等情况）再将脏数据批量写入数据库。是异步的，可以结合批量合并写

优点：响应快，写只更缓存，高吞吐，批量写数据库

缺点：数据可能丢失，存在数据不一致

适用于写操作多，如高写入负载（如日志采集、实时统计）。允许数据最终一致性（如社交媒体的点赞数）。

| **算法**               | **一致性强度** | **性能**     | **复杂度** | **典型场景**       |
| :--------------------- | :------------- | :----------- | :--------- | :----------------- |
| **Cache-Aside**        | 最终一致       | 读高，写中   | 低         | 电商、内容展示     |
| **Read/Write-Through** | 强一致         | 读高，写低   | 中         | 账户系统、配置管理 |
| **Write-Behind**       | 最终一致       | 读高，写极高 | 中         | 日志系统、实时指标 |



#### 大key

原因：对象太大(String)；容器太大(list set等)

排查：BIGKEYS命令，会输出每个类型key中最大的信息，最大String的字节数，最大list的元素个数，以及每种key占了多打内存

解决方法：对于String，客户端侧做一下压缩

对于集合：可以哈希分片，如点赞存所有用户id，可以id取余存到几个不同的key里

#### 热点缓存怎么处理

已知热点数据的话预热

redis集群，请求分散打到多个缓存服务器

​	如果根据key算哈希，一直都打到一个服务器，可以给key随机加点东西，让哈希之后能到不同服务器，要保证不同服务器存的都是一样的

多级缓存，本地缓存

冷热分离，防止业务都受影响

### todo

​		

命令：setnx key val

​			expire key time

​			ttl key

exist key

del key

mset

mget

lpush

lpop

rpush

rpop

zadd score member

zrem

zscore mem



补：常用指令 (set k y)(get k)(exist k)(strlen k)(del k)(incr k)(incrby k 10)(decr k)(decrby k 10)

(EXPIRE k 60 / SET k v EX 60)(TTL k)

(set lock_key unique_value EX PX 10000分布式锁，不存在就插入，过期时间10s)

补：pv访问次数（page view），uv(unique visitor)访问人数

save 3600 1 -- 3600 秒内有1个key被修改，触发RDB
save 300 100 -- 300 秒内有100个key被修改，触发RDB
save 60 10000 -- 60 秒内有10000个key被修改，触发RDB

分布式锁 set kv nx ex 10(因为setnx和过期时间需要同时成功或失败，需要原子操作，只能写在一条语句里)

